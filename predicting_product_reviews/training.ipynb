{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Sentiment in Product Reviews\n",
    "\n",
    "This project predicts whether a product review is **positive** or **negative** using a fine-tuned language model.\n",
    "\n",
    "**Dataset**: [Amazon Reviews Dataset](https://www.kaggle.com/datasets/kritanjalijain/amazon-reviews).\n",
    "\n",
    "**Model**: fine-tuned [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, \n",
    "    DistilBertForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Stuning even for the non-gamer</td>\n",
       "      <td>This sound track was beautiful! It paints the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The best soundtrack ever to anything.</td>\n",
       "      <td>I'm reading a lot of reviews saying that this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Amazing!</td>\n",
       "      <td>This soundtrack is my favorite music of all ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Excellent Soundtrack</td>\n",
       "      <td>I truly like this soundtrack and I enjoy video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>If you've played the game, you know how divine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599995</th>\n",
       "      <td>1</td>\n",
       "      <td>Don't do it!!</td>\n",
       "      <td>The high chair looks great when it first comes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599996</th>\n",
       "      <td>1</td>\n",
       "      <td>Looks nice, low functionality</td>\n",
       "      <td>I have used this highchair for 2 kids now and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599997</th>\n",
       "      <td>1</td>\n",
       "      <td>compact, but hard to clean</td>\n",
       "      <td>We have a small house, and really wanted two o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599998</th>\n",
       "      <td>1</td>\n",
       "      <td>what is it saying?</td>\n",
       "      <td>not sure what this book is supposed to be. It ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599999</th>\n",
       "      <td>2</td>\n",
       "      <td>Makes My Blood Run Red-White-And-Blue</td>\n",
       "      <td>I agree that every American should read this b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3600000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0                                                  1  \\\n",
       "0        2                     Stuning even for the non-gamer   \n",
       "1        2              The best soundtrack ever to anything.   \n",
       "2        2                                           Amazing!   \n",
       "3        2                               Excellent Soundtrack   \n",
       "4        2  Remember, Pull Your Jaw Off The Floor After He...   \n",
       "...     ..                                                ...   \n",
       "3599995  1                                      Don't do it!!   \n",
       "3599996  1                      Looks nice, low functionality   \n",
       "3599997  1                         compact, but hard to clean   \n",
       "3599998  1                                 what is it saying?   \n",
       "3599999  2              Makes My Blood Run Red-White-And-Blue   \n",
       "\n",
       "                                                         2  \n",
       "0        This sound track was beautiful! It paints the ...  \n",
       "1        I'm reading a lot of reviews saying that this ...  \n",
       "2        This soundtrack is my favorite music of all ti...  \n",
       "3        I truly like this soundtrack and I enjoy video...  \n",
       "4        If you've played the game, you know how divine...  \n",
       "...                                                    ...  \n",
       "3599995  The high chair looks great when it first comes...  \n",
       "3599996  I have used this highchair for 2 kids now and ...  \n",
       "3599997  We have a small house, and really wanted two o...  \n",
       "3599998  not sure what this book is supposed to be. It ...  \n",
       "3599999  I agree that every American should read this b...  \n",
       "\n",
       "[3600000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Dataset\n",
    "# Download and extract the archive into the 'dataset/' directory:\n",
    "# \"Amazon Reviews Dataset\" - https://www.kaggle.com/datasets/kritanjalijain/amazon-reviews\n",
    "def read_df(path):\n",
    "  df = None\n",
    "  try:\n",
    "      df = pd.read_csv(path, header=None)\n",
    "      print(\"Dataset loaded successfully!\")\n",
    "  except FileNotFoundError:\n",
    "      print(\"Error: The dataset file was not found. Please ensure it is in the 'dataset/' directory.\")\n",
    "\n",
    "  return df\n",
    "\n",
    "df = read_df('dataset/train.csv')\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1249535</th>\n",
       "      <td>0</td>\n",
       "      <td>Very poor:Basically a movie about a bunch of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2394083</th>\n",
       "      <td>1</td>\n",
       "      <td>Julie Garwood is a true master of writing. Eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729193</th>\n",
       "      <td>0</td>\n",
       "      <td>The VSI series has produced some terrific intr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769249</th>\n",
       "      <td>0</td>\n",
       "      <td>One of Australia's greatest anthropologists, W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155431</th>\n",
       "      <td>1</td>\n",
       "      <td>Very innovative for the time. The show was dra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         labels                                               text\n",
       "1249535       0  Very poor:Basically a movie about a bunch of t...\n",
       "2394083       1  Julie Garwood is a true master of writing. Eve...\n",
       "729193        0  The VSI series has produced some terrific intr...\n",
       "769249        0  One of Australia's greatest anthropologists, W...\n",
       "155431        1  Very innovative for the time. The show was dra..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set column names for the dataframe using info from the dataset web page\n",
    "# 'labels' is the target variable for Hugging Face Trainer, \n",
    "def prepare_dataframe(df):\n",
    "  col_names = ['labels', 'title', 'text']\n",
    "  df.columns = col_names\n",
    "\n",
    "  # Map labels: 1 (negative) -> 0, 2 (positive) -> 1\n",
    "  df['labels'] = df['labels'].map({1: 0, 2: 1})\n",
    "\n",
    "  # Train only on the text data\n",
    "  df = df.drop(['title'], axis=1)\n",
    "  return df\n",
    "\n",
    "df = prepare_dataframe(df)\n",
    "df.sample(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Test and Validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data values:\n",
      "0    150000\n",
      "1    150000\n",
      "Name: labels, dtype: int64\n",
      "Validation data values:\n",
      "1    30000\n",
      "0    30000\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Due to the main dataset size, use only fraction of samples\n",
    "# Set the number of samples for training and validation\n",
    "train_samples = 150000  # Number of training samples per class\n",
    "val_samples = 30000     # Number of validation samples per class\n",
    "random_state = 2        # Random state for reproducibility\n",
    "\n",
    "# Sample and split data into training and validation sets\n",
    "samples_pos = df[df['labels'] == 1].sample(train_samples + val_samples, random_state=random_state).reset_index(drop=True)\n",
    "samples_neg = df[df['labels'] == 0].sample(train_samples + val_samples, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "train_data = pd.concat([samples_pos[:train_samples], samples_neg[:train_samples]], ignore_index=True)\n",
    "train_data = train_data.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "val_data = pd.concat([samples_pos[train_samples:], samples_neg[train_samples:]], ignore_index=True)\n",
    "val_data = val_data.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "# Display class distributions\n",
    "print(f\"Training data values:\\n{train_data['labels'].value_counts()}\")\n",
    "print(f\"Validation data values:\\n{val_data['labels'].value_counts()}\")\n",
    "\n",
    "# Convert the data into Hugging Face Datasets\n",
    "hf_train_dataset = Dataset.from_pandas(train_data)\n",
    "hf_eval_dataset = Dataset.from_pandas(val_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: This is a great product!\n",
      "Tokenized output: {'input_ids': tensor([[ 101, 2023, 2003, 1037, 2307, 4031,  999,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Define label-to-ID and ID-to-label mappings for easier interpretation\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}  # Maps label ID to string labels\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}  # Maps string labels to label IDs\n",
    "\n",
    "# Load the pre-trained DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Load the pre-trained DistilBERT model with sequence classification head\n",
    "# Using num_labels=2 because we have two classes: \"POSITIVE\" and \"NEGATIVE\"\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", \n",
    "    num_labels=2, \n",
    "    id2label=id2label, \n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Test Tokenizer\n",
    "text = \"This is a great product!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "print(f'Input text: {text}')\n",
    "print(f'Tokenized output: {inputs}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a06778d49244983abc6b2264b038995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2cf79d1ffc450392f6278cadf59a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': 0, 'text': 'Item purchased in Dec. 2009. Finish looked fine initially. The finish began showing spots soon after I received it and after about 15 months it is now pitted.', 'input_ids': [101, 8875, 4156, 1999, 11703, 1012, 2268, 1012, 3926, 2246, 2986, 3322, 1012, 1996, 3926, 2211, 4760, 7516, 2574, 2044, 1045, 2363, 2009, 1998, 2044, 2055, 2321, 2706, 2009, 2003, 2085, 25895, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "def tok_func(x): return tokenizer(x[\"text\"], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "train_dataset = hf_train_dataset.map(tok_func, batched=True)\n",
    "eval_dataset = hf_eval_dataset.map(tok_func, batched=True)\n",
    "\n",
    "# Print sample\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for training\n",
    "bs = 16  # Batch size\n",
    "epochs = 2  # Number of epochs\n",
    "lr = 8e-5  # Learning rate\n",
    "\n",
    "# Set training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir='outputs',  # Directory to save model checkpoints\n",
    "    learning_rate=lr,  # Learning rate\n",
    "    warmup_ratio=0.1,  # Ratio of warmup steps\n",
    "    lr_scheduler_type='cosine',  # Type of learning rate scheduler\n",
    "    fp16=True,  # Enable mixed precision\n",
    "    eval_strategy=\"epoch\",  # Evaluation strategy: evaluate at the end of each epoch\n",
    "    per_device_train_batch_size=bs,  # Batch size per device during training\n",
    "    per_device_eval_batch_size=bs,  # Batch size per device during evaluation\n",
    "    num_train_epochs=epochs,  # Number of epochs to train\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    report_to='none'  # Disable logging to external services\n",
    ")\n",
    "\n",
    "# Function to compute evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc}  # Return accuracy as a dictionary\n",
    "\n",
    "# Initialize Trainer for model training\n",
    "trainer = Trainer(\n",
    "    model=model,  # Model to be trained\n",
    "    args=args,  # Training arguments\n",
    "    train_dataset=train_dataset,  # Training dataset\n",
    "    eval_dataset=eval_dataset,  # Evaluation dataset\n",
    "    processing_class=tokenizer,  # Tokenizer for text preprocessing\n",
    "    compute_metrics=compute_metrics  # Function to compute evaluation metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deae38084f1547658012f968d1e972a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4252, 'grad_norm': 7.222513198852539, 'learning_rate': 1.0624e-05, 'epoch': 0.03}\n",
      "{'loss': 0.2701, 'grad_norm': 2.50805401802063, 'learning_rate': 2.129066666666667e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2477, 'grad_norm': 8.326554298400879, 'learning_rate': 3.195733333333334e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2443, 'grad_norm': 8.237428665161133, 'learning_rate': 4.262400000000001e-05, 'epoch': 0.11}\n",
      "{'loss': 0.2659, 'grad_norm': 3.3289244174957275, 'learning_rate': 5.3290666666666675e-05, 'epoch': 0.13}\n",
      "{'loss': 0.239, 'grad_norm': 1.0574089288711548, 'learning_rate': 6.395733333333333e-05, 'epoch': 0.16}\n",
      "{'loss': 0.2384, 'grad_norm': 11.564011573791504, 'learning_rate': 7.460266666666667e-05, 'epoch': 0.19}\n",
      "{'loss': 0.2413, 'grad_norm': 7.556215763092041, 'learning_rate': 7.998942800469363e-05, 'epoch': 0.21}\n",
      "{'loss': 0.2393, 'grad_norm': 2.0158541202545166, 'learning_rate': 7.99033396418314e-05, 'epoch': 0.24}\n",
      "{'loss': 0.2312, 'grad_norm': 9.818373680114746, 'learning_rate': 7.973082953086285e-05, 'epoch': 0.27}\n",
      "{'loss': 0.2319, 'grad_norm': 1.9757444858551025, 'learning_rate': 7.94728739415097e-05, 'epoch': 0.29}\n",
      "{'loss': 0.2303, 'grad_norm': 18.146892547607422, 'learning_rate': 7.912899784501161e-05, 'epoch': 0.32}\n",
      "{'loss': 0.2415, 'grad_norm': 5.695108413696289, 'learning_rate': 7.8700377052311e-05, 'epoch': 0.35}\n",
      "{'loss': 0.2358, 'grad_norm': 6.524155139923096, 'learning_rate': 7.818793986060085e-05, 'epoch': 0.37}\n",
      "{'loss': 0.2266, 'grad_norm': 4.96926736831665, 'learning_rate': 7.759406809625822e-05, 'epoch': 0.4}\n",
      "{'loss': 0.2207, 'grad_norm': 1.0518487691879272, 'learning_rate': 7.691766813627341e-05, 'epoch': 0.43}\n",
      "{'loss': 0.2201, 'grad_norm': 5.630959987640381, 'learning_rate': 7.616131272782282e-05, 'epoch': 0.45}\n",
      "{'loss': 0.2285, 'grad_norm': 3.848893165588379, 'learning_rate': 7.532663996829818e-05, 'epoch': 0.48}\n",
      "{'loss': 0.2287, 'grad_norm': 0.37676411867141724, 'learning_rate': 7.441545757303413e-05, 'epoch': 0.51}\n",
      "{'loss': 0.225, 'grad_norm': 4.435905933380127, 'learning_rate': 7.343178339203342e-05, 'epoch': 0.53}\n",
      "{'loss': 0.2246, 'grad_norm': 3.4732346534729004, 'learning_rate': 7.237380597578284e-05, 'epoch': 0.56}\n",
      "{'loss': 0.2216, 'grad_norm': 4.375736236572266, 'learning_rate': 7.124571410469228e-05, 'epoch': 0.59}\n",
      "{'loss': 0.2228, 'grad_norm': 0.20155006647109985, 'learning_rate': 7.004995097455437e-05, 'epoch': 0.61}\n",
      "{'loss': 0.2218, 'grad_norm': 3.0010688304901123, 'learning_rate': 6.879169119051762e-05, 'epoch': 0.64}\n",
      "{'loss': 0.2226, 'grad_norm': 7.325659275054932, 'learning_rate': 6.746861764990994e-05, 'epoch': 0.67}\n",
      "{'loss': 0.2184, 'grad_norm': 3.334672212600708, 'learning_rate': 6.608887573151115e-05, 'epoch': 0.69}\n",
      "{'loss': 0.2161, 'grad_norm': 3.2645785808563232, 'learning_rate': 6.464992462163044e-05, 'epoch': 0.72}\n",
      "{'loss': 0.215, 'grad_norm': 7.934689998626709, 'learning_rate': 6.315758726399175e-05, 'epoch': 0.75}\n",
      "{'loss': 0.2038, 'grad_norm': 5.124319076538086, 'learning_rate': 6.161509572900244e-05, 'epoch': 0.77}\n",
      "{'loss': 0.1966, 'grad_norm': 1.9588111639022827, 'learning_rate': 6.0025790709849866e-05, 'epoch': 0.8}\n",
      "{'loss': 0.2051, 'grad_norm': 3.1814639568328857, 'learning_rate': 5.839311428730371e-05, 'epoch': 0.83}\n",
      "{'loss': 0.2181, 'grad_norm': 1.010627269744873, 'learning_rate': 5.672060247493568e-05, 'epoch': 0.85}\n",
      "{'loss': 0.209, 'grad_norm': 3.440451145172119, 'learning_rate': 5.501187756090137e-05, 'epoch': 0.88}\n",
      "{'loss': 0.2135, 'grad_norm': 4.172115802764893, 'learning_rate': 5.327415268918843e-05, 'epoch': 0.91}\n",
      "{'loss': 0.2176, 'grad_norm': 2.2949509620666504, 'learning_rate': 5.150779381838452e-05, 'epoch': 0.93}\n",
      "{'loss': 0.1932, 'grad_norm': 2.3808398246765137, 'learning_rate': 4.9712999317390036e-05, 'epoch': 0.96}\n",
      "{'loss': 0.1954, 'grad_norm': 11.054341316223145, 'learning_rate': 4.789716862273612e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55fd07714d9493b823d0336d67ba948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21029998362064362, 'eval_accuracy': 0.9292333333333334, 'eval_runtime': 138.1795, 'eval_samples_per_second': 434.218, 'eval_steps_per_second': 27.139, 'epoch': 1.0}\n",
      "{'loss': 0.1804, 'grad_norm': 10.898591995239258, 'learning_rate': 4.6064234419363564e-05, 'epoch': 1.01}\n",
      "{'loss': 0.1723, 'grad_norm': 10.273165702819824, 'learning_rate': 4.4221869024504e-05, 'epoch': 1.04}\n",
      "{'loss': 0.1576, 'grad_norm': 12.35455322265625, 'learning_rate': 4.236667969698144e-05, 'epoch': 1.07}\n",
      "{'loss': 0.1628, 'grad_norm': 0.5494061708450317, 'learning_rate': 4.0506364668240464e-05, 'epoch': 1.09}\n",
      "{'loss': 0.1514, 'grad_norm': 9.472646713256836, 'learning_rate': 3.864495296638388e-05, 'epoch': 1.12}\n",
      "{'loss': 0.1548, 'grad_norm': 0.13523219525814056, 'learning_rate': 3.678647599466424e-05, 'epoch': 1.15}\n",
      "{'loss': 0.1565, 'grad_norm': 3.466481924057007, 'learning_rate': 3.4934958800362105e-05, 'epoch': 1.17}\n",
      "{'loss': 0.1473, 'grad_norm': 10.717649459838867, 'learning_rate': 3.3094411357429956e-05, 'epoch': 1.2}\n",
      "{'loss': 0.1547, 'grad_norm': 2.2358856201171875, 'learning_rate': 3.1268819881781505e-05, 'epoch': 1.23}\n",
      "{'loss': 0.1557, 'grad_norm': 0.4030872881412506, 'learning_rate': 2.9465730081003055e-05, 'epoch': 1.25}\n",
      "{'loss': 0.1476, 'grad_norm': 17.87855339050293, 'learning_rate': 2.7681821540464446e-05, 'epoch': 1.28}\n",
      "{'loss': 0.1469, 'grad_norm': 9.148547172546387, 'learning_rate': 2.5924591431493574e-05, 'epoch': 1.31}\n",
      "{'loss': 0.1487, 'grad_norm': 10.54868221282959, 'learning_rate': 2.420126609429935e-05, 'epoch': 1.33}\n",
      "{'loss': 0.1488, 'grad_norm': 9.261313438415527, 'learning_rate': 2.2508672001606798e-05, 'epoch': 1.36}\n",
      "{'loss': 0.1492, 'grad_norm': 0.17981703579425812, 'learning_rate': 2.085396023044143e-05, 'epoch': 1.39}\n",
      "{'loss': 0.1327, 'grad_norm': 5.888046741485596, 'learning_rate': 1.9240714518033314e-05, 'epoch': 1.41}\n",
      "{'loss': 0.1528, 'grad_norm': 9.287912368774414, 'learning_rate': 1.76724287953627e-05, 'epoch': 1.44}\n",
      "{'loss': 0.145, 'grad_norm': 11.798727035522461, 'learning_rate': 1.615249962008365e-05, 'epoch': 1.47}\n",
      "{'loss': 0.1407, 'grad_norm': 0.09799595177173615, 'learning_rate': 1.4684218820336514e-05, 'epoch': 1.49}\n",
      "{'loss': 0.1499, 'grad_norm': 0.24137195944786072, 'learning_rate': 1.3270766365381067e-05, 'epoch': 1.52}\n",
      "{'loss': 0.1445, 'grad_norm': 11.222363471984863, 'learning_rate': 1.1915203478491323e-05, 'epoch': 1.55}\n",
      "{'loss': 0.1343, 'grad_norm': 0.3222402334213257, 'learning_rate': 1.0622992891851954e-05, 'epoch': 1.57}\n",
      "{'loss': 0.1403, 'grad_norm': 2.465341567993164, 'learning_rate': 9.394152165009034e-06, 'epoch': 1.6}\n",
      "{'loss': 0.1371, 'grad_norm': 0.734032154083252, 'learning_rate': 8.229069749990265e-06, 'epoch': 1.63}\n",
      "{'loss': 0.1348, 'grad_norm': 3.252568244934082, 'learning_rate': 7.132796095236014e-06, 'epoch': 1.65}\n",
      "{'loss': 0.1402, 'grad_norm': 5.435586452484131, 'learning_rate': 6.107705485367512e-06, 'epoch': 1.68}\n",
      "{'loss': 0.1459, 'grad_norm': 4.580921649932861, 'learning_rate': 5.156018038381585e-06, 'epoch': 1.71}\n",
      "{'loss': 0.1368, 'grad_norm': 4.7802276611328125, 'learning_rate': 4.279794897369427e-06, 'epoch': 1.73}\n",
      "{'loss': 0.1383, 'grad_norm': 8.6134614944458, 'learning_rate': 3.4824531032532137e-06, 'epoch': 1.76}\n",
      "{'loss': 0.1434, 'grad_norm': 0.19183136522769928, 'learning_rate': 2.7625243401691216e-06, 'epoch': 1.79}\n",
      "{'loss': 0.1543, 'grad_norm': 31.317895889282227, 'learning_rate': 2.1232436574330296e-06, 'epoch': 1.81}\n",
      "{'loss': 0.1345, 'grad_norm': 10.165949821472168, 'learning_rate': 1.5659955946506045e-06, 'epoch': 1.84}\n",
      "{'loss': 0.1298, 'grad_norm': 4.2202935218811035, 'learning_rate': 1.0919870270394806e-06, 'epoch': 1.87}\n",
      "{'loss': 0.129, 'grad_norm': 5.062089920043945, 'learning_rate': 7.022445516062748e-07, 'epoch': 1.89}\n",
      "{'loss': 0.12, 'grad_norm': 7.131292819976807, 'learning_rate': 3.976122637660762e-07, 'epoch': 1.92}\n",
      "{'loss': 0.1345, 'grad_norm': 6.878532409667969, 'learning_rate': 1.7874992921972145e-07, 'epoch': 1.95}\n",
      "{'loss': 0.1507, 'grad_norm': 0.2344236820936203, 'learning_rate': 4.6131555048218865e-08, 'epoch': 1.97}\n",
      "{'loss': 0.1281, 'grad_norm': 0.6910629272460938, 'learning_rate': 4.436311896061085e-11, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554919ca0528428ca10538a21c382d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18882055580615997, 'eval_accuracy': 0.94455, 'eval_runtime': 140.3408, 'eval_samples_per_second': 427.531, 'eval_steps_per_second': 26.721, 'epoch': 2.0}\n",
      "{'train_runtime': 5731.7244, 'train_samples_per_second': 104.681, 'train_steps_per_second': 6.543, 'train_loss': 0.1877202207438151, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=37500, training_loss=0.1877202207438151, metrics={'train_runtime': 5731.7244, 'train_samples_per_second': 104.681, 'train_steps_per_second': 6.543, 'total_flos': 5.165557103457062e+16, 'train_loss': 0.1877202207438151, 'epoch': 2.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the test dataset\n",
    "test_df = read_df('dataset/test.csv')\n",
    "test_df = prepare_dataframe(test_df)\n",
    "\n",
    "samples_test = pd.concat([test_df[test_df['labels'] == 0].sample(25000, random_state=random_state), \n",
    "                          test_df[test_df['labels'] == 1].sample(25000, random_state=random_state)])\n",
    "\n",
    "hf_test_dataset = Dataset.from_pandas(samples_test)\n",
    "\n",
    "test_dataset = hf_test_dataset.map(tok_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347ac05f603d4bbfbae008b6940fcc4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the model using the test dataset\n",
    "test_results = trainer.predict(test_dataset)\n",
    "print(test_results.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"outputs/checkpoint-37500\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9973002076148987}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''Great Product!'''\n",
    "classifier(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
